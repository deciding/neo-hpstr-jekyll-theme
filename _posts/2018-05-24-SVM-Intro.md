### Day4

**SVM**

**logistic regression**: from logistic regression to SVM. LR is going to get the line across two categories, and convert the linear function to sigmoid. SVM is to get the hyperplane perpendicular to that line, when y(est)=1 if wx+b>0, -1 if wx+b<0, and only considering support vectors. can also think SVM as LR, with linear expression >0 -> y(est)=1, expression<0 -> y(est)=-1

**hyperplane**: wx+b=0(w*(x-x0)=0)

**functional margin, geometrical margin**: y(wx+b), y(wx+b)/||w||, functional margin will change when w b change with same ratio, although they are same hyperplane. We can use this property to tune functional margin with fixed geometrical margin

**maximum margin classifer(objective function)** : we let functional margin be 1, then geometrical margin is 1/||w||, the objective function is *max 1/||w||, st y(i)(wx(i)+b)>=1*, since it has restriction, we can think of lagrange duality

**linear classifiable: duality**:  convert *max 1/||w||, st y(i)(wx(i)+b)>=1* to *min ||w||^/2, st y(i)(wx(i)+b)>=1*, this is **non-linear programming**, and is also **QP**(qudratic programming), we can use **KKT** condition to solve it. 

1. use KKT condition 2 to get the partial derivative of L(w,b,a) on w and b, we can get **w**=sum(ai y(i) **x**(i)), sum(ai y(i))=0 -> L(**wn**,b,**am**)=sum(m,ai)-sum(m,m,aiajyiyj**xi**T**xj**)

2. use SMO to solve L st. KKT1 and aL/ab=0->sum(aiyi)=0 from KKT2 and KKT3 convert to get max of simplified L.

3. since L is only with **a** parameters, we can use **w**=sum(m,aiyixi)to get **w** and b=-(max neg margin+min pos margin)/2 to get b

use duality because:

1. easier to calculate

2. can introduce kernel

**support vector**: is the vector on the hyperplane, since they are the actual vectors needed to determine the hyperplace wx+b=0, bcoz their a is 0. also non support vector make yi(wxi-b)-1>0, thus to maximize L(w,b,a), ai must be 0. 


**perceptron**: a bit similar to SVM. but it minimize the wrong classification distance instead. L(w,b)=sum(M, -yi(wxi+b)), where M are set with wrong classifications

<https://blog.csdn.net/denghecsdn/article/details/77313758> hyperplane

<https://blog.csdn.net/alwaystry/article/details/60957096> SVM

<https://www.cnblogs.com/hapjin/p/6714526.html> perceptron

### Day5, Day6

**non linear classifier**: quadratic curve means X1^2+X1+X1X2+X2^2+X2 2d->5d, 3d->19d... dimension explode

**kernel function**: use f=(<x1 . x2> +1)^2 is very like the inner product of higher dimension, we use it as kernel function to manipulate inner product in higher dimension, thus will avoid dimension problem, and do the classification in lower dimension, *L=sum(m,ai)-sum(m,m,aiajyiyjf(xi,xj))*

**outlier**: use hyperparameter to make the error included in L also. objective function=*min 1/2*||w||^2+ C*sum(ei) st. ei>=0, yi(wxi+b)>=1-ei*, ei is slack variable

L(w,b,e,a,r)=*min 1/2*||w||^2+ C*sum(ei)-sum(ai(yi(wxi_b)-1+ei))-sum(riei)*

duality problem: aL/aei->C-ai-ri=0 *max sum(ai) -1/2 sum(aiajyiyj<xi.xj>) st 0<=ai<=C, sum(aiyi)=0*

**proof of SVM(and SMO)** not currently...

**grid search**: w*(a,b, other hyperparams) is hard to use normal optimization methods, since it may not be convex. thus we use (a1, a2, ...) (b1,b2,...) as grid to get the min of the pair

<https://www.csie.ntu.edu.tw/~cjlin/libsvm/> svm online tool

