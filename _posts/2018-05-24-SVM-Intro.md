### Day4

**SVM**

**logistic regression**: from logistic regression to SVM. LR is going to get the line across two categories, and convert the linear function to sigmoid. SVM is to get the hyperplane perpendicular to that line, when y(est)=1 if wx+b>0, -1 if wx+b<0, and only considering support vectors. can also think SVM as LR, with linear expression >0 -> y(est)=1, expression<0 -> y(est)=-1

**hyperplane**: wx+b=0(w*(x-x0)=0)

**functional margin, geometrical margin**: y(wx+b), y(wx+b)/||w||, functional margin will change when w b change with same ratio, although they are same hyperplane. We can use this property to tune functional margin with fixed geometrical margin

**maximum margin classifer(objective function)** : we let functional margin be 1, then geometrical margin is 1/||w||, the objective function is *max 1/||w||, st y(i)(wx(i)+b)>=1*, since it has restriction, we can think of lagrange duality

**linear classifiable: duality**:  convert *max 1/||w||, st y(i)(wx(i)+b)>=1* to *min ||w||^/2, st y(i)(wx(i)+b)>=1*, this is **non-linear programming**, and is also **QP**(qudratic programming), we can use **KKT** condition to solve it. 

1. use KKT condition 2 to get the partial derivative of L(w,b,a) on w and b, we can get **w**=sum(ai y(i) **x**(i)), sum(ai y(i))=0 -> L(**wn**,b,**am**)=sum(m,ai)-sum(m,m,aiajyiyj**xi**T**xi**)

2. use SMO to solve L st. KKT1 and aL/ab from KKT2 and KKT3 convert to get max of simplified L.

3. since L is only with **a** parameters, we can use **w**=sum(m,aiyixi)to get **w** and b=-(max neg margin+min pos margin)/2 to get b

use duality because:

1. easier to calculate

2. can introduce kernel

**support vector**: is the vector on the hyperplane, since they are the actual vectors needed to determine the hyperplace wx+b=0, bcoz their a is 0. also non support vector make yi(wxi-b)-1>0, thus to maximize L(w,b,a), ai must be 0. 

<https://blog.csdn.net/denghecsdn/article/details/77313758> hyperplane

<https://blog.csdn.net/alwaystry/article/details/60957096> SVM
