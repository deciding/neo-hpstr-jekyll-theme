### Day10

**NN**: perceptron -> MLP(back propagation) -> DL (pre-training,fine-tuning)

active function(sign, sigmoid, ReLu), neuron, hidden layer, input/ouput layer, output is the non linear active function of linear combination of input

**CNN**: local receptive fields, shared weight -> less params. convolution layer, activation layer(ReLu), pooling layer(max, avg)

**RNN**: input size is unpredicatable, we are interested in a sequence of input. x(t), h(t) hidden, o(t) output, L(t) loss function, y(t) predicted. h(t)=g1(Ux(t)+Wh(t-1)+b), o(t)=Vh(t)+c, y(t)=g2(o(t))

<https://www.cnblogs.com/subconscious/p/5058741.html> NN

<https://blog.csdn.net/cxmscb/article/details/71023576> CNN

<https://www.cnblogs.com/pinard/p/6509630.html>RNN
